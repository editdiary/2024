{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":386,"status":"ok","timestamp":1718077625229,"user":{"displayName":"이도현","userId":"02134394201878625870"},"user_tz":-540},"id":"kwxuBAj7YAMN"},"outputs":[],"source":["# 필요 라이브러리 import\n","import os, sys\n","import random\n","import time\n","import gc\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torchvision\n","import torchvision.models as models\n","from torchvision import datasets, transforms\n","\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchsummary import summary\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from PIL import Image\n","from tqdm import tqdm\n","from timm import create_model"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1718077337671,"user":{"displayName":"이도현","userId":"02134394201878625870"},"user_tz":-540},"id":"z4Bgd947YAMQ"},"outputs":[],"source":["# 데이터 경로 및 SEED 설정\n","DATA_DIR = \"dataset\"\n","SEED = 0xC0FFEE     # 12648430\n","\n","# 재현성을 위한 시드 고정\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"rE9tzwpwYAMR"},"source":["# 데이터 로드"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"H4zLr3bqYAMU"},"outputs":[],"source":["# 이미지 전처리 정의\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),                                    # 흑백 이미지 load (output = 3ch)\n","    transforms.Resize((224, 224)),                                                  # 224x224 resize\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # ImageNet 정규화\n","    ])\n","\n","# ImageFolder를 사용하여 데이터셋 만들기\n","original_dataset = datasets.ImageFolder(root=\"augmented_trainset\")\n","\n","# train/validation 분리\n","train_size = len(original_dataset) - 800\n","val_size = 800\n","train_dataset, val_dataset = random_split(original_dataset, [train_size, val_size])\n","\n","# transform 적용\n","train_dataset.dataset.transform = transform\n","val_dataset.dataset.transform = transform"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# DataLoader 인스턴스 생성\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n","validation_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"C2tBwAIyYAMU","outputId":"71838104-94bf-4526-93ab-6a229c5d63a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 3, 224, 224]) torch.Size([32])\n","torch.Size([1, 3, 224, 224]) torch.Size([1])\n"]}],"source":["# 데이터 형태 확인\n","x, y = next(iter(train_loader))\n","print(x.shape, y.shape)\n","\n","x, y = next(iter(validation_loader))\n","print(x.shape, y.shape)"]},{"cell_type":"markdown","metadata":{"id":"4a0ZrHwZYAMV"},"source":["# Label 확인"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IakgIALLYAMV","outputId":"819bb66f-d3b1-4eea-c389-03007cffb62a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['aphids', 'armyworm', 'blisterbeetle', 'cicadellidae', 'cornborer', 'cricket', 'delicatula', 'limacodidae', 'miridae', 'viridis']\n","{0: 'aphids', 1: 'armyworm', 2: 'blisterbeetle', 3: 'cicadellidae', 4: 'cornborer', 5: 'cricket', 6: 'delicatula', 7: 'limacodidae', 8: 'miridae', 9: 'viridis'}\n"]}],"source":["classes = original_dataset.classes\n","print(classes)\n","\n","idx_to_class = {v: k for k, v in original_dataset.class_to_idx.items()}\n","print(idx_to_class)"]},{"cell_type":"markdown","metadata":{"id":"2HApjpMIYAMW"},"source":["# 모델 정의"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VJbfuDQWYAMW","outputId":"25383b3c-067e-4b02-d8b3-911a3fd7add2"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["# CUDA 사용 가능 여부 확인\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2Q7ZQBPFYAMX"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f423c86be8924573b9af621576e32144","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\prohe\\anaconda3\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prohe\\.cache\\huggingface\\hub\\models--timm--swin_large_patch4_window7_224.ms_in22k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]}],"source":["# 모델 정의\n","class SwinModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SwinModel, self).__init__()\n","        self.swin = create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=num_classes)\n","\n","    def forward(self, x):\n","        return self.swin(x)\n","    \n","model = SwinModel(num_classes=len(classes)).to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"iO7AqhZBYAMY","outputId":"710d47e9-b364-4e53-d71b-4ec34a402089"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 192, 56, 56]           9,408\n","         LayerNorm-2          [-1, 56, 56, 192]             384\n","        PatchEmbed-3          [-1, 56, 56, 192]               0\n","          Identity-4          [-1, 56, 56, 192]               0\n","         LayerNorm-5          [-1, 56, 56, 192]             384\n","            Linear-6              [-1, 49, 576]         111,168\n","           Softmax-7            [-1, 6, 49, 49]               0\n","           Dropout-8            [-1, 6, 49, 49]               0\n","            Linear-9              [-1, 49, 192]          37,056\n","          Dropout-10              [-1, 49, 192]               0\n","  WindowAttention-11              [-1, 49, 192]               0\n","         Identity-12          [-1, 56, 56, 192]               0\n","        LayerNorm-13            [-1, 3136, 192]             384\n","           Linear-14            [-1, 3136, 768]         148,224\n","             GELU-15            [-1, 3136, 768]               0\n","          Dropout-16            [-1, 3136, 768]               0\n","         Identity-17            [-1, 3136, 768]               0\n","           Linear-18            [-1, 3136, 192]         147,648\n","          Dropout-19            [-1, 3136, 192]               0\n","              Mlp-20            [-1, 3136, 192]               0\n","         Identity-21            [-1, 3136, 192]               0\n","SwinTransformerBlock-22          [-1, 56, 56, 192]               0\n","        LayerNorm-23          [-1, 56, 56, 192]             384\n","           Linear-24              [-1, 49, 576]         111,168\n","          Softmax-25            [-1, 6, 49, 49]               0\n","          Dropout-26            [-1, 6, 49, 49]               0\n","           Linear-27              [-1, 49, 192]          37,056\n","          Dropout-28              [-1, 49, 192]               0\n","  WindowAttention-29              [-1, 49, 192]               0\n","         DropPath-30          [-1, 56, 56, 192]               0\n","        LayerNorm-31            [-1, 3136, 192]             384\n","           Linear-32            [-1, 3136, 768]         148,224\n","             GELU-33            [-1, 3136, 768]               0\n","          Dropout-34            [-1, 3136, 768]               0\n","         Identity-35            [-1, 3136, 768]               0\n","           Linear-36            [-1, 3136, 192]         147,648\n","          Dropout-37            [-1, 3136, 192]               0\n","              Mlp-38            [-1, 3136, 192]               0\n","         DropPath-39            [-1, 3136, 192]               0\n","SwinTransformerBlock-40          [-1, 56, 56, 192]               0\n","SwinTransformerStage-41          [-1, 56, 56, 192]               0\n","        LayerNorm-42          [-1, 28, 28, 768]           1,536\n","           Linear-43          [-1, 28, 28, 384]         294,912\n","     PatchMerging-44          [-1, 28, 28, 384]               0\n","        LayerNorm-45          [-1, 28, 28, 384]             768\n","           Linear-46             [-1, 49, 1152]         443,520\n","          Softmax-47           [-1, 12, 49, 49]               0\n","          Dropout-48           [-1, 12, 49, 49]               0\n","           Linear-49              [-1, 49, 384]         147,840\n","          Dropout-50              [-1, 49, 384]               0\n","  WindowAttention-51              [-1, 49, 384]               0\n","         DropPath-52          [-1, 28, 28, 384]               0\n","        LayerNorm-53             [-1, 784, 384]             768\n","           Linear-54            [-1, 784, 1536]         591,360\n","             GELU-55            [-1, 784, 1536]               0\n","          Dropout-56            [-1, 784, 1536]               0\n","         Identity-57            [-1, 784, 1536]               0\n","           Linear-58             [-1, 784, 384]         590,208\n","          Dropout-59             [-1, 784, 384]               0\n","              Mlp-60             [-1, 784, 384]               0\n","         DropPath-61             [-1, 784, 384]               0\n","SwinTransformerBlock-62          [-1, 28, 28, 384]               0\n","        LayerNorm-63          [-1, 28, 28, 384]             768\n","           Linear-64             [-1, 49, 1152]         443,520\n","          Softmax-65           [-1, 12, 49, 49]               0\n","          Dropout-66           [-1, 12, 49, 49]               0\n","           Linear-67              [-1, 49, 384]         147,840\n","          Dropout-68              [-1, 49, 384]               0\n","  WindowAttention-69              [-1, 49, 384]               0\n","         DropPath-70          [-1, 28, 28, 384]               0\n","        LayerNorm-71             [-1, 784, 384]             768\n","           Linear-72            [-1, 784, 1536]         591,360\n","             GELU-73            [-1, 784, 1536]               0\n","          Dropout-74            [-1, 784, 1536]               0\n","         Identity-75            [-1, 784, 1536]               0\n","           Linear-76             [-1, 784, 384]         590,208\n","          Dropout-77             [-1, 784, 384]               0\n","              Mlp-78             [-1, 784, 384]               0\n","         DropPath-79             [-1, 784, 384]               0\n","SwinTransformerBlock-80          [-1, 28, 28, 384]               0\n","SwinTransformerStage-81          [-1, 28, 28, 384]               0\n","        LayerNorm-82         [-1, 14, 14, 1536]           3,072\n","           Linear-83          [-1, 14, 14, 768]       1,179,648\n","     PatchMerging-84          [-1, 14, 14, 768]               0\n","        LayerNorm-85          [-1, 14, 14, 768]           1,536\n","           Linear-86             [-1, 49, 2304]       1,771,776\n","          Softmax-87           [-1, 24, 49, 49]               0\n","          Dropout-88           [-1, 24, 49, 49]               0\n","           Linear-89              [-1, 49, 768]         590,592\n","          Dropout-90              [-1, 49, 768]               0\n","  WindowAttention-91              [-1, 49, 768]               0\n","         DropPath-92          [-1, 14, 14, 768]               0\n","        LayerNorm-93             [-1, 196, 768]           1,536\n","           Linear-94            [-1, 196, 3072]       2,362,368\n","             GELU-95            [-1, 196, 3072]               0\n","          Dropout-96            [-1, 196, 3072]               0\n","         Identity-97            [-1, 196, 3072]               0\n","           Linear-98             [-1, 196, 768]       2,360,064\n","          Dropout-99             [-1, 196, 768]               0\n","             Mlp-100             [-1, 196, 768]               0\n","        DropPath-101             [-1, 196, 768]               0\n","SwinTransformerBlock-102          [-1, 14, 14, 768]               0\n","       LayerNorm-103          [-1, 14, 14, 768]           1,536\n","          Linear-104             [-1, 49, 2304]       1,771,776\n","         Softmax-105           [-1, 24, 49, 49]               0\n","         Dropout-106           [-1, 24, 49, 49]               0\n","          Linear-107              [-1, 49, 768]         590,592\n","         Dropout-108              [-1, 49, 768]               0\n"," WindowAttention-109              [-1, 49, 768]               0\n","        DropPath-110          [-1, 14, 14, 768]               0\n","       LayerNorm-111             [-1, 196, 768]           1,536\n","          Linear-112            [-1, 196, 3072]       2,362,368\n","            GELU-113            [-1, 196, 3072]               0\n","         Dropout-114            [-1, 196, 3072]               0\n","        Identity-115            [-1, 196, 3072]               0\n","          Linear-116             [-1, 196, 768]       2,360,064\n","         Dropout-117             [-1, 196, 768]               0\n","             Mlp-118             [-1, 196, 768]               0\n","        DropPath-119             [-1, 196, 768]               0\n","SwinTransformerBlock-120          [-1, 14, 14, 768]               0\n","       LayerNorm-121          [-1, 14, 14, 768]           1,536\n","          Linear-122             [-1, 49, 2304]       1,771,776\n","         Softmax-123           [-1, 24, 49, 49]               0\n","         Dropout-124           [-1, 24, 49, 49]               0\n","          Linear-125              [-1, 49, 768]         590,592\n","         Dropout-126              [-1, 49, 768]               0\n"," WindowAttention-127              [-1, 49, 768]               0\n","        DropPath-128          [-1, 14, 14, 768]               0\n","       LayerNorm-129             [-1, 196, 768]           1,536\n","          Linear-130            [-1, 196, 3072]       2,362,368\n","            GELU-131            [-1, 196, 3072]               0\n","         Dropout-132            [-1, 196, 3072]               0\n","        Identity-133            [-1, 196, 3072]               0\n","          Linear-134             [-1, 196, 768]       2,360,064\n","         Dropout-135             [-1, 196, 768]               0\n","             Mlp-136             [-1, 196, 768]               0\n","        DropPath-137             [-1, 196, 768]               0\n","SwinTransformerBlock-138          [-1, 14, 14, 768]               0\n","       LayerNorm-139          [-1, 14, 14, 768]           1,536\n","          Linear-140             [-1, 49, 2304]       1,771,776\n","         Softmax-141           [-1, 24, 49, 49]               0\n","         Dropout-142           [-1, 24, 49, 49]               0\n","          Linear-143              [-1, 49, 768]         590,592\n","         Dropout-144              [-1, 49, 768]               0\n"," WindowAttention-145              [-1, 49, 768]               0\n","        DropPath-146          [-1, 14, 14, 768]               0\n","       LayerNorm-147             [-1, 196, 768]           1,536\n","          Linear-148            [-1, 196, 3072]       2,362,368\n","            GELU-149            [-1, 196, 3072]               0\n","         Dropout-150            [-1, 196, 3072]               0\n","        Identity-151            [-1, 196, 3072]               0\n","          Linear-152             [-1, 196, 768]       2,360,064\n","         Dropout-153             [-1, 196, 768]               0\n","             Mlp-154             [-1, 196, 768]               0\n","        DropPath-155             [-1, 196, 768]               0\n","SwinTransformerBlock-156          [-1, 14, 14, 768]               0\n","       LayerNorm-157          [-1, 14, 14, 768]           1,536\n","          Linear-158             [-1, 49, 2304]       1,771,776\n","         Softmax-159           [-1, 24, 49, 49]               0\n","         Dropout-160           [-1, 24, 49, 49]               0\n","          Linear-161              [-1, 49, 768]         590,592\n","         Dropout-162              [-1, 49, 768]               0\n"," WindowAttention-163              [-1, 49, 768]               0\n","        DropPath-164          [-1, 14, 14, 768]               0\n","       LayerNorm-165             [-1, 196, 768]           1,536\n","          Linear-166            [-1, 196, 3072]       2,362,368\n","            GELU-167            [-1, 196, 3072]               0\n","         Dropout-168            [-1, 196, 3072]               0\n","        Identity-169            [-1, 196, 3072]               0\n","          Linear-170             [-1, 196, 768]       2,360,064\n","         Dropout-171             [-1, 196, 768]               0\n","             Mlp-172             [-1, 196, 768]               0\n","        DropPath-173             [-1, 196, 768]               0\n","SwinTransformerBlock-174          [-1, 14, 14, 768]               0\n","       LayerNorm-175          [-1, 14, 14, 768]           1,536\n","          Linear-176             [-1, 49, 2304]       1,771,776\n","         Softmax-177           [-1, 24, 49, 49]               0\n","         Dropout-178           [-1, 24, 49, 49]               0\n","          Linear-179              [-1, 49, 768]         590,592\n","         Dropout-180              [-1, 49, 768]               0\n"," WindowAttention-181              [-1, 49, 768]               0\n","        DropPath-182          [-1, 14, 14, 768]               0\n","       LayerNorm-183             [-1, 196, 768]           1,536\n","          Linear-184            [-1, 196, 3072]       2,362,368\n","            GELU-185            [-1, 196, 3072]               0\n","         Dropout-186            [-1, 196, 3072]               0\n","        Identity-187            [-1, 196, 3072]               0\n","          Linear-188             [-1, 196, 768]       2,360,064\n","         Dropout-189             [-1, 196, 768]               0\n","             Mlp-190             [-1, 196, 768]               0\n","        DropPath-191             [-1, 196, 768]               0\n","SwinTransformerBlock-192          [-1, 14, 14, 768]               0\n","       LayerNorm-193          [-1, 14, 14, 768]           1,536\n","          Linear-194             [-1, 49, 2304]       1,771,776\n","         Softmax-195           [-1, 24, 49, 49]               0\n","         Dropout-196           [-1, 24, 49, 49]               0\n","          Linear-197              [-1, 49, 768]         590,592\n","         Dropout-198              [-1, 49, 768]               0\n"," WindowAttention-199              [-1, 49, 768]               0\n","        DropPath-200          [-1, 14, 14, 768]               0\n","       LayerNorm-201             [-1, 196, 768]           1,536\n","          Linear-202            [-1, 196, 3072]       2,362,368\n","            GELU-203            [-1, 196, 3072]               0\n","         Dropout-204            [-1, 196, 3072]               0\n","        Identity-205            [-1, 196, 3072]               0\n","          Linear-206             [-1, 196, 768]       2,360,064\n","         Dropout-207             [-1, 196, 768]               0\n","             Mlp-208             [-1, 196, 768]               0\n","        DropPath-209             [-1, 196, 768]               0\n","SwinTransformerBlock-210          [-1, 14, 14, 768]               0\n","       LayerNorm-211          [-1, 14, 14, 768]           1,536\n","          Linear-212             [-1, 49, 2304]       1,771,776\n","         Softmax-213           [-1, 24, 49, 49]               0\n","         Dropout-214           [-1, 24, 49, 49]               0\n","          Linear-215              [-1, 49, 768]         590,592\n","         Dropout-216              [-1, 49, 768]               0\n"," WindowAttention-217              [-1, 49, 768]               0\n","        DropPath-218          [-1, 14, 14, 768]               0\n","       LayerNorm-219             [-1, 196, 768]           1,536\n","          Linear-220            [-1, 196, 3072]       2,362,368\n","            GELU-221            [-1, 196, 3072]               0\n","         Dropout-222            [-1, 196, 3072]               0\n","        Identity-223            [-1, 196, 3072]               0\n","          Linear-224             [-1, 196, 768]       2,360,064\n","         Dropout-225             [-1, 196, 768]               0\n","             Mlp-226             [-1, 196, 768]               0\n","        DropPath-227             [-1, 196, 768]               0\n","SwinTransformerBlock-228          [-1, 14, 14, 768]               0\n","       LayerNorm-229          [-1, 14, 14, 768]           1,536\n","          Linear-230             [-1, 49, 2304]       1,771,776\n","         Softmax-231           [-1, 24, 49, 49]               0\n","         Dropout-232           [-1, 24, 49, 49]               0\n","          Linear-233              [-1, 49, 768]         590,592\n","         Dropout-234              [-1, 49, 768]               0\n"," WindowAttention-235              [-1, 49, 768]               0\n","        DropPath-236          [-1, 14, 14, 768]               0\n","       LayerNorm-237             [-1, 196, 768]           1,536\n","          Linear-238            [-1, 196, 3072]       2,362,368\n","            GELU-239            [-1, 196, 3072]               0\n","         Dropout-240            [-1, 196, 3072]               0\n","        Identity-241            [-1, 196, 3072]               0\n","          Linear-242             [-1, 196, 768]       2,360,064\n","         Dropout-243             [-1, 196, 768]               0\n","             Mlp-244             [-1, 196, 768]               0\n","        DropPath-245             [-1, 196, 768]               0\n","SwinTransformerBlock-246          [-1, 14, 14, 768]               0\n","       LayerNorm-247          [-1, 14, 14, 768]           1,536\n","          Linear-248             [-1, 49, 2304]       1,771,776\n","         Softmax-249           [-1, 24, 49, 49]               0\n","         Dropout-250           [-1, 24, 49, 49]               0\n","          Linear-251              [-1, 49, 768]         590,592\n","         Dropout-252              [-1, 49, 768]               0\n"," WindowAttention-253              [-1, 49, 768]               0\n","        DropPath-254          [-1, 14, 14, 768]               0\n","       LayerNorm-255             [-1, 196, 768]           1,536\n","          Linear-256            [-1, 196, 3072]       2,362,368\n","            GELU-257            [-1, 196, 3072]               0\n","         Dropout-258            [-1, 196, 3072]               0\n","        Identity-259            [-1, 196, 3072]               0\n","          Linear-260             [-1, 196, 768]       2,360,064\n","         Dropout-261             [-1, 196, 768]               0\n","             Mlp-262             [-1, 196, 768]               0\n","        DropPath-263             [-1, 196, 768]               0\n","SwinTransformerBlock-264          [-1, 14, 14, 768]               0\n","       LayerNorm-265          [-1, 14, 14, 768]           1,536\n","          Linear-266             [-1, 49, 2304]       1,771,776\n","         Softmax-267           [-1, 24, 49, 49]               0\n","         Dropout-268           [-1, 24, 49, 49]               0\n","          Linear-269              [-1, 49, 768]         590,592\n","         Dropout-270              [-1, 49, 768]               0\n"," WindowAttention-271              [-1, 49, 768]               0\n","        DropPath-272          [-1, 14, 14, 768]               0\n","       LayerNorm-273             [-1, 196, 768]           1,536\n","          Linear-274            [-1, 196, 3072]       2,362,368\n","            GELU-275            [-1, 196, 3072]               0\n","         Dropout-276            [-1, 196, 3072]               0\n","        Identity-277            [-1, 196, 3072]               0\n","          Linear-278             [-1, 196, 768]       2,360,064\n","         Dropout-279             [-1, 196, 768]               0\n","             Mlp-280             [-1, 196, 768]               0\n","        DropPath-281             [-1, 196, 768]               0\n","SwinTransformerBlock-282          [-1, 14, 14, 768]               0\n","       LayerNorm-283          [-1, 14, 14, 768]           1,536\n","          Linear-284             [-1, 49, 2304]       1,771,776\n","         Softmax-285           [-1, 24, 49, 49]               0\n","         Dropout-286           [-1, 24, 49, 49]               0\n","          Linear-287              [-1, 49, 768]         590,592\n","         Dropout-288              [-1, 49, 768]               0\n"," WindowAttention-289              [-1, 49, 768]               0\n","        DropPath-290          [-1, 14, 14, 768]               0\n","       LayerNorm-291             [-1, 196, 768]           1,536\n","          Linear-292            [-1, 196, 3072]       2,362,368\n","            GELU-293            [-1, 196, 3072]               0\n","         Dropout-294            [-1, 196, 3072]               0\n","        Identity-295            [-1, 196, 3072]               0\n","          Linear-296             [-1, 196, 768]       2,360,064\n","         Dropout-297             [-1, 196, 768]               0\n","             Mlp-298             [-1, 196, 768]               0\n","        DropPath-299             [-1, 196, 768]               0\n","SwinTransformerBlock-300          [-1, 14, 14, 768]               0\n","       LayerNorm-301          [-1, 14, 14, 768]           1,536\n","          Linear-302             [-1, 49, 2304]       1,771,776\n","         Softmax-303           [-1, 24, 49, 49]               0\n","         Dropout-304           [-1, 24, 49, 49]               0\n","          Linear-305              [-1, 49, 768]         590,592\n","         Dropout-306              [-1, 49, 768]               0\n"," WindowAttention-307              [-1, 49, 768]               0\n","        DropPath-308          [-1, 14, 14, 768]               0\n","       LayerNorm-309             [-1, 196, 768]           1,536\n","          Linear-310            [-1, 196, 3072]       2,362,368\n","            GELU-311            [-1, 196, 3072]               0\n","         Dropout-312            [-1, 196, 3072]               0\n","        Identity-313            [-1, 196, 3072]               0\n","          Linear-314             [-1, 196, 768]       2,360,064\n","         Dropout-315             [-1, 196, 768]               0\n","             Mlp-316             [-1, 196, 768]               0\n","        DropPath-317             [-1, 196, 768]               0\n","SwinTransformerBlock-318          [-1, 14, 14, 768]               0\n","       LayerNorm-319          [-1, 14, 14, 768]           1,536\n","          Linear-320             [-1, 49, 2304]       1,771,776\n","         Softmax-321           [-1, 24, 49, 49]               0\n","         Dropout-322           [-1, 24, 49, 49]               0\n","          Linear-323              [-1, 49, 768]         590,592\n","         Dropout-324              [-1, 49, 768]               0\n"," WindowAttention-325              [-1, 49, 768]               0\n","        DropPath-326          [-1, 14, 14, 768]               0\n","       LayerNorm-327             [-1, 196, 768]           1,536\n","          Linear-328            [-1, 196, 3072]       2,362,368\n","            GELU-329            [-1, 196, 3072]               0\n","         Dropout-330            [-1, 196, 3072]               0\n","        Identity-331            [-1, 196, 3072]               0\n","          Linear-332             [-1, 196, 768]       2,360,064\n","         Dropout-333             [-1, 196, 768]               0\n","             Mlp-334             [-1, 196, 768]               0\n","        DropPath-335             [-1, 196, 768]               0\n","SwinTransformerBlock-336          [-1, 14, 14, 768]               0\n","       LayerNorm-337          [-1, 14, 14, 768]           1,536\n","          Linear-338             [-1, 49, 2304]       1,771,776\n","         Softmax-339           [-1, 24, 49, 49]               0\n","         Dropout-340           [-1, 24, 49, 49]               0\n","          Linear-341              [-1, 49, 768]         590,592\n","         Dropout-342              [-1, 49, 768]               0\n"," WindowAttention-343              [-1, 49, 768]               0\n","        DropPath-344          [-1, 14, 14, 768]               0\n","       LayerNorm-345             [-1, 196, 768]           1,536\n","          Linear-346            [-1, 196, 3072]       2,362,368\n","            GELU-347            [-1, 196, 3072]               0\n","         Dropout-348            [-1, 196, 3072]               0\n","        Identity-349            [-1, 196, 3072]               0\n","          Linear-350             [-1, 196, 768]       2,360,064\n","         Dropout-351             [-1, 196, 768]               0\n","             Mlp-352             [-1, 196, 768]               0\n","        DropPath-353             [-1, 196, 768]               0\n","SwinTransformerBlock-354          [-1, 14, 14, 768]               0\n","       LayerNorm-355          [-1, 14, 14, 768]           1,536\n","          Linear-356             [-1, 49, 2304]       1,771,776\n","         Softmax-357           [-1, 24, 49, 49]               0\n","         Dropout-358           [-1, 24, 49, 49]               0\n","          Linear-359              [-1, 49, 768]         590,592\n","         Dropout-360              [-1, 49, 768]               0\n"," WindowAttention-361              [-1, 49, 768]               0\n","        DropPath-362          [-1, 14, 14, 768]               0\n","       LayerNorm-363             [-1, 196, 768]           1,536\n","          Linear-364            [-1, 196, 3072]       2,362,368\n","            GELU-365            [-1, 196, 3072]               0\n","         Dropout-366            [-1, 196, 3072]               0\n","        Identity-367            [-1, 196, 3072]               0\n","          Linear-368             [-1, 196, 768]       2,360,064\n","         Dropout-369             [-1, 196, 768]               0\n","             Mlp-370             [-1, 196, 768]               0\n","        DropPath-371             [-1, 196, 768]               0\n","SwinTransformerBlock-372          [-1, 14, 14, 768]               0\n","       LayerNorm-373          [-1, 14, 14, 768]           1,536\n","          Linear-374             [-1, 49, 2304]       1,771,776\n","         Softmax-375           [-1, 24, 49, 49]               0\n","         Dropout-376           [-1, 24, 49, 49]               0\n","          Linear-377              [-1, 49, 768]         590,592\n","         Dropout-378              [-1, 49, 768]               0\n"," WindowAttention-379              [-1, 49, 768]               0\n","        DropPath-380          [-1, 14, 14, 768]               0\n","       LayerNorm-381             [-1, 196, 768]           1,536\n","          Linear-382            [-1, 196, 3072]       2,362,368\n","            GELU-383            [-1, 196, 3072]               0\n","         Dropout-384            [-1, 196, 3072]               0\n","        Identity-385            [-1, 196, 3072]               0\n","          Linear-386             [-1, 196, 768]       2,360,064\n","         Dropout-387             [-1, 196, 768]               0\n","             Mlp-388             [-1, 196, 768]               0\n","        DropPath-389             [-1, 196, 768]               0\n","SwinTransformerBlock-390          [-1, 14, 14, 768]               0\n","       LayerNorm-391          [-1, 14, 14, 768]           1,536\n","          Linear-392             [-1, 49, 2304]       1,771,776\n","         Softmax-393           [-1, 24, 49, 49]               0\n","         Dropout-394           [-1, 24, 49, 49]               0\n","          Linear-395              [-1, 49, 768]         590,592\n","         Dropout-396              [-1, 49, 768]               0\n"," WindowAttention-397              [-1, 49, 768]               0\n","        DropPath-398          [-1, 14, 14, 768]               0\n","       LayerNorm-399             [-1, 196, 768]           1,536\n","          Linear-400            [-1, 196, 3072]       2,362,368\n","            GELU-401            [-1, 196, 3072]               0\n","         Dropout-402            [-1, 196, 3072]               0\n","        Identity-403            [-1, 196, 3072]               0\n","          Linear-404             [-1, 196, 768]       2,360,064\n","         Dropout-405             [-1, 196, 768]               0\n","             Mlp-406             [-1, 196, 768]               0\n","        DropPath-407             [-1, 196, 768]               0\n","SwinTransformerBlock-408          [-1, 14, 14, 768]               0\n","SwinTransformerStage-409          [-1, 14, 14, 768]               0\n","       LayerNorm-410           [-1, 7, 7, 3072]           6,144\n","          Linear-411           [-1, 7, 7, 1536]       4,718,592\n","    PatchMerging-412           [-1, 7, 7, 1536]               0\n","       LayerNorm-413           [-1, 7, 7, 1536]           3,072\n","          Linear-414             [-1, 49, 4608]       7,082,496\n","         Softmax-415           [-1, 48, 49, 49]               0\n","         Dropout-416           [-1, 48, 49, 49]               0\n","          Linear-417             [-1, 49, 1536]       2,360,832\n","         Dropout-418             [-1, 49, 1536]               0\n"," WindowAttention-419             [-1, 49, 1536]               0\n","        DropPath-420           [-1, 7, 7, 1536]               0\n","       LayerNorm-421             [-1, 49, 1536]           3,072\n","          Linear-422             [-1, 49, 6144]       9,443,328\n","            GELU-423             [-1, 49, 6144]               0\n","         Dropout-424             [-1, 49, 6144]               0\n","        Identity-425             [-1, 49, 6144]               0\n","          Linear-426             [-1, 49, 1536]       9,438,720\n","         Dropout-427             [-1, 49, 1536]               0\n","             Mlp-428             [-1, 49, 1536]               0\n","        DropPath-429             [-1, 49, 1536]               0\n","SwinTransformerBlock-430           [-1, 7, 7, 1536]               0\n","       LayerNorm-431           [-1, 7, 7, 1536]           3,072\n","          Linear-432             [-1, 49, 4608]       7,082,496\n","         Softmax-433           [-1, 48, 49, 49]               0\n","         Dropout-434           [-1, 48, 49, 49]               0\n","          Linear-435             [-1, 49, 1536]       2,360,832\n","         Dropout-436             [-1, 49, 1536]               0\n"," WindowAttention-437             [-1, 49, 1536]               0\n","        DropPath-438           [-1, 7, 7, 1536]               0\n","       LayerNorm-439             [-1, 49, 1536]           3,072\n","          Linear-440             [-1, 49, 6144]       9,443,328\n","            GELU-441             [-1, 49, 6144]               0\n","         Dropout-442             [-1, 49, 6144]               0\n","        Identity-443             [-1, 49, 6144]               0\n","          Linear-444             [-1, 49, 1536]       9,438,720\n","         Dropout-445             [-1, 49, 1536]               0\n","             Mlp-446             [-1, 49, 1536]               0\n","        DropPath-447             [-1, 49, 1536]               0\n","SwinTransformerBlock-448           [-1, 7, 7, 1536]               0\n","SwinTransformerStage-449           [-1, 7, 7, 1536]               0\n","       LayerNorm-450           [-1, 7, 7, 1536]           3,072\n","FastAdaptiveAvgPool-451                 [-1, 1536]               0\n","        Identity-452                 [-1, 1536]               0\n","SelectAdaptivePool2d-453                 [-1, 1536]               0\n","         Dropout-454                 [-1, 1536]               0\n","          Linear-455                   [-1, 10]          15,370\n","        Identity-456                   [-1, 10]               0\n","  ClassifierHead-457                   [-1, 10]               0\n"," SwinTransformer-458                   [-1, 10]               0\n","================================================================\n","Total params: 194,915,530\n","Trainable params: 194,915,530\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 959.27\n","Params size (MB): 743.54\n","Estimated Total Size (MB): 1703.39\n","----------------------------------------------------------------\n"]}],"source":["# 모델 확인\n","summary(model, (3, 224, 224))"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ow7h3KVTYAMY"},"outputs":[],"source":["# 손실 함수와 최적화 함수 정의\n","optimizer = optim.Adam(model.parameters(), lr=1e-6)     # 1e-6 = 0.000001\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"E8wBrGDpYAMY"},"source":["# 모델 훈련 및 검증"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PVTETyM2YAMZ"},"outputs":[],"source":["# 학습 함수 정의\n","def fit(model, data_loader, loss_fn, optimizer, device, phase='train'):\n","    # phase에 따라 모델의 모드 설정\n","    if phase == 'train':\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    running_loss = 0.0\n","    running_corrects = 0\n","\n","    # tqdm을 사용하여 반복문 진행 상황 시각화\n","    prograss_bar = tqdm(data_loader, leave=False)\n","\n","    # mini-batch 단위 학습 시작\n","    for img, lbl in prograss_bar:\n","        img, lbl = img.to(device), lbl.to(device)\n","\n","        optimizer.zero_grad()           # 누적 Gradient 초기화\n","\n","        # Gradient 계산을 통한 Forward Propagation\n","        with torch.set_grad_enabled(phase == 'train'):\n","            pred = model(img)           # Forward Propagation 수행\n","            loss = loss_fn(pred, lbl)   # 손실 값 계산\n","\n","            if phase == 'train':        # 학습 모드인 경우 Backward Propagation 및 가중치 업데이트 수행\n","                loss.backward()\n","                optimizer.step()\n","\n","        pred = pred.argmax(1)           # pred의 확률값을 클래스 레이블로 변환\n","        running_loss += loss.item()     # 손실 값 누적\n","        running_corrects += torch.sum(pred == lbl.data)    # 정답 수 누적\n","\n","    # 손실 값과 정확도 계산\n","    final_acc = running_corrects / len(data_loader.dataset)\n","    final_loss = running_loss / len(data_loader.dataset)\n","\n","    return final_loss, final_acc"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ptBchdZYYAMZ","outputId":"93b59c43-ebb3-4074-b9b5-03a8e94fc008"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                   \r"]},{"name":"stdout","output_type":"stream","text":["[INFO] val_loss has been improved from inf to 0.04715. Saving Model!\n","[Epoch01] time: 4m 25s \t loss: 0.00014, acc: 0.99932 | val_loss: 0.04715, val_acc: 0.98750\n"]},{"name":"stderr","output_type":"stream","text":["                                                   \r"]},{"name":"stdout","output_type":"stream","text":["[INFO] val_loss has been improved from 0.04715 to 0.04395. Saving Model!\n","[Epoch02] time: 4m 24s \t loss: 0.00009, acc: 0.99960 | val_loss: 0.04395, val_acc: 0.99000\n"]},{"name":"stderr","output_type":"stream","text":["                                                \r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 학습 및 검증 단계 진행\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m fit(\n\u001b[1;32m     20\u001b[0m     model, validation_loader, loss_fn, optimizer, device, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 학습 결과 기록\u001b[39;00m\n","Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, phase)\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, lbl)   \u001b[38;5;66;03m# 손실 값 계산\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:        \u001b[38;5;66;03m# 학습 모드인 경우 Backward Propagation 및 가중치 업데이트 수행\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)           \u001b[38;5;66;03m# pred의 확률값을 클래스 레이블로 변환\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Epoch별 모델 학습\n","num_epochs = 30\n","\n","min_loss = np.inf\n","max_acc = 0.0\n","\n","record_train_loss, record_train_acc = [], []\n","record_valid_loss, record_valid_acc = [], []\n","\n","STATE_DICT_PATH = \"augmented_SwinLarge.pth\"\n","\n","for epoch in range(num_epochs):\n","    start = time.time()\n","\n","    # 학습 및 검증 단계 진행\n","    train_loss, train_acc = fit(\n","        model, train_loader, loss_fn, optimizer, device, phase='train'\n","    )\n","    valid_loss, valid_acc = fit(\n","        model, validation_loader, loss_fn, optimizer, device, phase='valid'\n","    )\n","\n","    # 학습 결과 기록\n","    record_train_loss.append(train_loss)\n","    record_train_acc.append(train_acc)\n","    record_valid_loss.append(valid_loss)\n","    record_valid_acc.append(valid_acc)\n","\n","    # 성능이 좋아질 경우 모델 저장\n","    if valid_loss < min_loss:\n","        print(\n","            f\"[INFO] val_loss has been improved from {min_loss:.5f} to {valid_loss:.5f}. Saving Model!\"\n","        )\n","        min_loss = valid_loss\n","        torch.save(model.state_dict(), STATE_DICT_PATH)\n","\n","    # 학습 시간 및 손실 값, 정확도 출력\n","    time_elapsed = time.time() - start\n","    print(\n","        f\"[Epoch{epoch+1:02d}] time: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s \\t loss: {train_loss:.5f}, acc: {train_acc:.5f} | val_loss: {valid_loss:.5f}, val_acc: {valid_acc:.5f}\"\n","    )"]},{"cell_type":"markdown","metadata":{"id":"aEaZj5u6bW-i"},"source":["# 검증 정확도 확인"]},{"cell_type":"markdown","metadata":{"id":"NvUjCgH1YAMZ"},"source":["> 저장한 모델의 가중치 load"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"alte7hNvYAMZ","outputId":"d50bf8d7-f759-4dc1-a67a-18769d4ac255"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(\"augmented_SwinLarge.pth\"))    # 가장 성능이 좋았던 모델 불러오기"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# validation 데이터에 대한 정답값 확인\n","y_true = []\n","y_pred = []\n","\n","model.eval()\n","for img, lbl in validation_loader:\n","    img, lbl = img.to(device), lbl.to(device)\n","\n","    with torch.no_grad():\n","        pred = model(img)\n","\n","    pred = pred.argmax(1)\n","    y_true.extend(lbl.cpu().numpy())\n","    y_pred.extend(pred.cpu().numpy())"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"T2aimElJYAMa"},"outputs":[],"source":["predictons = []\n","model = model.to(device)\n","model.eval()\n","\n","with torch.no_grad():\n","    running_loss = 0.0\n","    running_corrects = 0\n","\n","    for img, lbl in validation_loader:\n","        img, lbl = img.to(device), lbl.to(device)\n","\n","        pred = model(img)\n","        loss = loss_fn(pred, lbl)\n","\n","        running_loss += loss.item()\n","        running_corrects += torch.sum(pred.argmax(1) == lbl.data)\n","\n","        predictons.extend(pred.argmax(1).cpu().numpy())     # extend: 리스트에 다른 리스트의 요소를 추가할 때 사용\n","\n","    # 손실 값과 정확도 계산\n","    final_acc = running_corrects / len(validation_loader.dataset)\n","    final_loss = running_loss / len(validation_loader.dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_3FJa-y4YAMa","outputId":"15b872f6-5112-412e-9c15-5c589f76f1c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["<<Final Performances>>  Loss: 0.04395 | Accuracy: 0.99000\n","Prediction Length: 800  |  Prediction Example: [4, 1, 9, 1, 0, 7, 2, 8, 1, 5, 4, 4, 5, 5, 6]\n"]}],"source":["# 결과 확인\n","print(f\"<<Final Performances>>  Loss: {final_loss:.5f} | Accuracy: {final_acc:.5f}\")\n","print(f\"Prediction Length: {len(predictons)}  |  Prediction Example: {predictons[:15]}\")"]},{"cell_type":"markdown","metadata":{"id":"AxQq59naYAMa"},"source":["# 최종 예측 수행"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1718077437636,"user":{"displayName":"이도현","userId":"02134394201878625870"},"user_tz":-540},"id":"sJW4rRZUZSqC","outputId":"1a552313-02a8-4c6c-9645-300ca24b6a2b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FilePath</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>./dataset/problemset/001.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>./dataset/problemset/002.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>./dataset/problemset/003.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>./dataset/problemset/004.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>./dataset/problemset/005.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       FilePath\n","0  ./dataset/problemset/001.jpg\n","1  ./dataset/problemset/002.jpg\n","2  ./dataset/problemset/003.jpg\n","3  ./dataset/problemset/004.jpg\n","4  ./dataset/problemset/005.jpg"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["problem = pd.read_csv(os.path.join(DATA_DIR, \"problem.csv\"))\n","problem.head()"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":402,"status":"ok","timestamp":1718077631351,"user":{"displayName":"이도현","userId":"02134394201878625870"},"user_tz":-540},"id":"PoOzktPFZS8f"},"outputs":[],"source":["class CustomImageDataset(Dataset):\n","    def __init__(self, dataframe, transform=None):\n","        self.dataframe = dataframe\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.dataframe.iloc[idx, 0]      # 'FilePath' 열에서 이미지 경로 가져오기\n","        image = Image.open(img_path).convert(\"L\")   # 이미지를 흑백으로 로드\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image\n","\n","\n","# 이미지를 불러올 때 적용할 전처리 정의: resize, to tensor\n","problem_transform = transforms.Compose(\n","    [\n","        transforms.Grayscale(num_output_channels=3),                                    # 흑백 이미지 load (output = 3ch)\n","        transforms.Resize((224, 224)),                                                  # 224x224 resize\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # ImageNet 정규화\n","    ]\n",")\n","\n","\n","# 커스텀 데이터셋 인스턴스 생성\n","custom_dataset = CustomImageDataset(dataframe=problem, transform=problem_transform)\n","\n","# DataLoader 인스턴스 생성\n","problem_loader = DataLoader(custom_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"1NcLavk4ZY50"},"outputs":[],"source":["predictions = []\n","\n","# 검증모드 진입\n","model.eval()\n","\n","with torch.no_grad():\n","    # loss 초기화\n","    running_loss = 0\n","    # 정확도 계산\n","    running_acc = 0\n","    for img in problem_loader:\n","        img = img.to(device)\n","\n","        y_hat = model(img)\n","        label = y_hat.argmax(dim=1).detach().item()\n","        predictions.append(label)\n","\n","# 숫자 라벨을 클래스 이름으로 변환\n","your_answer = [idx_to_class[l] for l in predictions]"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Ivaxf2CgZnie"},"outputs":[],"source":["submission = pd.read_csv(os.path.join(DATA_DIR, \"submission.csv\"))\n","submission[\"Label\"] = your_answer"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"bywL52gxZnlC"},"outputs":[],"source":["# 제출 파일 저장\n","submission.to_csv(\"augmented_SwinLarge.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcEBFrHmZS-x"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"py38","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
